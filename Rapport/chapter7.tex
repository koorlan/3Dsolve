Il se d√©gage des objectifs le besoin de pr√©senter √  l'utilisateur les solutions du casse-t√™te et de lui permettre d'interagir intuitivement avec l'application. Afin de r√©pondre √  ce besoin, il est n√©cessaire d'avoir un affichage volum√©trique (3d) et une interface permettant la lecture des entr√©es des p√©riph√©riques courants (clavier, souris). Nous avons donc d√©cid√© d'utiliser OpenGL (Open Graphics Library) comme librairie graphique et GLFW (GL FrameWork) comme librairie de gestion de fen√™tre et d'entr√©es.

Afin de permettre des calculs asynchrones √  la repr√©sentation graphique du cube serpent, nous avons d√©cid√© de cr√©er un thread d√©di√© √  cette repr√©sentation qui aurait pour but la simple lecture de donn√©es relatives au casse t√™te. En limitant la fr√©quence de rafraichissement de l'affichage √  60 images/secondes, le temps processeur est √©pargn√© de mani√®re √  favoriser les calculs relatifs √  la r√©solution,  la fluidit√© d'affichage est garantie, et les conflits de lecture deviennent tr√®s peu fr√©quents (et m√™me imperceptibles).

\section{OpenGL}

OpenGL (Open Graphics Library) est un ensemble normalis√© de fonctions de calcul d'images 2D ou 3D lanc√© par Silicon Graphics en 1992. Cette interface de programmation est disponible sur de nombreuses plateformes o√π elle est utilis√©e pour des applications qui vont du jeu vid√©o jusqu'√  la CAO en passant par la mod√©lisation.

\begin{quotation}
 OpenGL permet √  un programme de d√©clarer la g√©om√©trie d'objets sous forme de points, de vecteurs, de polygones, de bitmaps et de textures. OpenGL effectue ensuite des calculs de projection en vue de d√©terminer l'image √  l'√©cran, en tenant compte de la distance, de l'orientation, des ombres, de la transparence et du cadrage.\newline
 [source: http://fr.wikipedia.org/wiki/OpenGL]
\end{quotation}

Nous avons choisi de restreindre notre code √  la version 2.1 d'OpenGL pour des raisons de compatibilit√©, celle-ci √©tant support√©e par tous les pilotes de carte graphique r√©cents. Cette version apporte le concept de pipeline programmable: certaines parties de la cr√©ation d'image sont cod√©es par le d√©veloppeur, compil√©es et ex√©cut√©es par le GPU. Ces programmes sont appel√©s shaders et permettent un contr√¥le accru du proc√©d√© de calcul d'image.

\begin{figure}[h]
 \centering
 \includegraphics[scale=0.3,keepaspectratio=true]{img/pipeline.png}
 \caption{Le pipeline programmable (simplifi√©)}
 \label{pipeline}
\end{figure}

Les donn√©es (Vertices, UV, textures) et les shaders sont tous charg√©s dans la m√©moire graphique au d√©marrage du programme, permettant ainsi de r√©duire grandement le flux de donn√©es sur le bus graphique. Ainsi, seuls des identifiants (entiers) sont √  communiquer au GPU pour choisir sur quelles donn√©es sont ex√©cut√©es les op√©rations.
    La figure~\ref{pipeline} d√©taille les diff√©rentes √©tapes cl√©s du calcul d'une image au sein de la carte graphique:

\begin{enumerate}
 \item Dans le programme, on informe le GPU d'utiliser un VAO (Vertex Array Object), qui va contenir des informations relatives aux vertices (points dans l'espace). Dans notre cas, un VAO contient un tableau de vertices (x, y, z) et un tableau de coordonn√©es de texture (U, V) pour chaque vertex (ces tableaux sont appel√©s VBO, Vertex Buffer Object). Cet objet est trait√© en (1) et execute l'instruction suivante pour chaque vertex du VAO.
 \item Ces vertices sont ensuite trait√©s par le /vertex shader/ (2) (d√©taill√© par la suite), qui effectue des op√©rations de calcul vectoriel.
 \item Une fois transform√©s, ces vertices sont assembl√©s en (3) pour former des primitives. Dans notre cas, une primitive est un triangle, form√© par un triplet de vertices adjacents dans le premier tableau du VAO.
 \item Vient ensuite l'√©tape de rasterization (4): des fragments de pixels sont cr√©√©s √  partir des primitives. Un fragment correspond √  une subdivision surfacique de primitive de la taille d'un √©chantillon de pixel, c'est une conversion d'une information vectorielle vers une information √©chantillonn√©e.
 \item Les fragments ainsi cr√©√©s sont ensuite trait√©s par le /fragment shader/ (5) (d√©taill√© par la suite), qui va "coller" la texture renseign√©e par le programme sur le fragment en fonction de sa position sur la primitive.
 \item Les fragments ayant les m√™mes coordonn√©es √  l'√©cran sont ensuite ordonn√©s en fonction de la position de la primitive dont il est issu: un fragment issu d'une primitive cach√©e par une autre sera plac√© "derri√®re" celui issu de la primitive le cachant, quel que soit l'ordre dans lesquels ils ont √©t√© trait√©s pr√©alablement (6). Les primitives en arri√®re plan seront donc partiellement ou totalement occult√©es.
 \item Ces fragments ordonn√©es sont ensuite fusionn√©s en pixels (7). Cette √©tape r√©alise deux op√©rations majoritaires: la fusion de couleur des fragments (en fonction de leur transparence) en √©chantillons, et le rassemblement de ces √©chantillons pour former un pixel. L‚Äôint√©r√™t de poss√©der plusieurs √©chantillons est entre autres l'anti-cr√©nelage (lissage des traits). On peut dire qu'un pixel est issu de n fragments, avec n = nombre d'√©chantillons * nombre de primitives sous le pixel.
 \item Ces pixels sont ensuite convertis dans le format d'image sp√©cifi√© par l'application, ici d√©termin√© par le syst√®me d'exploitation, puis affich√©s √  l'√©cran.
\end{enumerate}

\section{Donn√©es}
Les donn√©es contenues dans les VAO et les textures sont issues de fichiers afin de faciliter leur cr√©ation et leur modification. On peut trouver les mod√®les 3D dans le dossier "stc" et les textures dans le dossier "textures".

Nous avons adopt√© deux formats de mod√®les 3D diff√©rents au cours du d√©veloppement, STC (format propre) et le format OBJ de WaveFront (tr√®s courant gr√¢ce √  sa simplicit√© de codage). Le format STC d√©crit les donn√©es telles qu'elles seront agenc√©es dans les VAOs tandis que le format OBJ est plus complexe, mais globalement plus pratique. Il contient une suite de vertices (pr√©fixe "v"), de vecteurs normaux (pr√©fixe "vn"), de coordonn√©es de texture (pr√©fixe "vt") et des descripteurs de primitives (pr√©fixe "f") associant pour chaque primitive trois triplets (vertex/normale/coordonn√©e de texture). Une fois d√©cod√©es, ces donn√©es sont transf√©r√©es dans la m√©moire graphique par les VAOs.

Pour les textures, nous avons choisi d'utiliser le format PNG, pour sa compression sans perte, son canal de transparence et son format largement document√©. La biblioth√®que lodePNG a √©t√© utilis√©e pour d√©coder ces textures de mani√®re simple. Une fois d√©cod√©es, elles sont transf√©r√©es dans la m√©moire graphique par des "texture buffers".

\section{Utilisation des donn√©es}
Les vertices des VAO ne contiennent que des coordonn√©es relatives au centre de l'objet, il est donc n√©cessaire de les transformer dans l'espace afin de les d√©placer, de les faire tourner et de changer leur √©chelle. Ces op√©rations sont r√©alis√©es au travers de calcul matriciel √  l'int√©rieur du /vertex shader/. En effet, les GPU sont optimis√©s afin de r√©aliser de nombreuses multiplications matricielles √  la cha√Æne. Voici une partie du code de notre vertex shader principal:\newline

\verb|gl_Position = VP * W * vec4 (vertex_position, 1.0);|\newline

Dans ce code on renseigne au GPU la nouvelle position de notre vertex, multipli√© par deux matrices. La premi√®re, VP (View-Projection) correspond √  la matrice de projection de l'espace en trois dimensions vers un espace en deux dimensions (Calcul√©e √  partir de la cam√©ra) et la deuxi√®me, W (World position) correspond √  la position de l'objet dont est issu le vertex dans cet espace en trois dimensions.

Ces matrices ne sont modifi√©s qu'une fois par image (alors que tous les vertices sont calcul√©s √  chaque image), il est donc judicieux de les calculer par le CPU et de les envoyer via le bus graphique. La matrice VP est calcul√©e en fonction de la cam√©ra, qui est modifi√©e par les actions de l'utilisateur. La matrice W est calcul√©e par rapport √  la position d'un cube dans notre espace en trois dimension virtuel.

Le fragment shader quant √  lui permet de "coller" une partie de texture pr√©alablement choisie sur un fragment. Cette op√©ration est effectu√©e gr√¢ce aux coordonn√©es de texture (UV): elles permettent de relier un vertex (x, y, z) √  un point d'une texture (u, v), ce qui permet de d√©former la texture en d√©formant la primitive associ√©e.

\begin{figure}[h]
 \centering
 \includegraphics[scale=0.4,keepaspectratio=true]{img/uvexpl.png}
 \caption{Explication UV}
 \label{uv}
\end{figure}

Voici une partie du code de notre fragment shader principal:\newline

\verb|gl_FragColor = vec4(texture2D( CurTex, UV ).rgb, alpha);|\newline

Dans ce code, on renseigne au GPU d'aller chercher une portion de texture √  la coordonn√©e (u, v), de lui ajouter une valeur de transparence arbitraire d√©finie dans notre programme et de l'associer au fragment trait√©.